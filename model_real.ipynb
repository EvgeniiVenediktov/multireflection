{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess_images import data_from_folder\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchsummary import summary\n",
    "import cv2 \n",
    "import wandb\n",
    "from config import LMDB_USE_COMPRESSION\n",
    "\n",
    "import lmdb\n",
    "import os\n",
    "import msgpack\n",
    "import lz4.frame\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data_from_folder(\"H:/latest_real_data/real_data/real\", grayscale=True, target_size=(125, 125))\n",
    "# data = data_from_folder(\"./data/laser_x4_y6\")\n",
    "# data = data_from_folder(\"./data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in data:\n",
    "#     print(k)\n",
    "#     plt.imshow(data[k], vmin=0, vmax=255)\n",
    "#     # plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imname_to_target(name:str) -> tuple[float]:\n",
    "    \"\"\"Parses image names of format x{x_value}_y{y_value}.jpg\"\"\"\n",
    "    name = name.split('.jpg')[0]\n",
    "    x, y = name.split(\"_\")\n",
    "    x = float(x[1:])\n",
    "    y = float(y[1:5])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"experiment_name\": \"mixed_002step_smallext1_512bs_00001lr_aug+\",\n",
    "    \"batch_size\": 512,\n",
    "    \"lr\": 0.001,\n",
    "    \"lr_scheduler_loop\": 7,\n",
    "    \"epochs\": 28,\n",
    "    \"use_amp\": False,\n",
    "\n",
    "    \"data_folder\": \"/mnt/h/real_512_0_001step.lmdb\",\n",
    "    \"dataset_type\": \"LMDBImageDataset\",\n",
    "    \"dataset_config_flatten\": False,\n",
    "    \"dataset_train_keys_fname\": \"002_mixed_keys_train.txt\",\n",
    "    \"dataset_val_keys_fname\": \"002_mixed_keys_val.txt\",\n",
    "\n",
    "    \"noise_level\": 0.1,\n",
    "    \"jitter_brightness\": 0.4, \n",
    "    \"jitter_contrast\": 0.1, \n",
    "    \"jitter_saturation\": 0.1, \n",
    "    \"jitter_hue\": 0.2,\n",
    "\n",
    "    \"data_collection_step\": 0.001,\n",
    "    \"starting_checkpoint_fname\": None,\n",
    "    \"checkpoint_folder\": \"./saved_models/real\",\n",
    "\n",
    "    \"conv_stride\": 2,\n",
    "    \"conv_kernel\": 3,\n",
    "    \"conv_depth\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image filenames for FilesImageDataset\n",
    "FilesImageDataset_fnames = []\n",
    "if config[\"dataset_type\"] == \"FilesImageDataset\":\n",
    "    for file in os.listdir(config[\"data_folder\"]):\n",
    "            if not file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                  continue\n",
    "            if config[\"data_collection_step\"] == 0.002:\n",
    "                # TODO: Attention! Mimmicking 0.002 step\n",
    "                x, y = imname_to_target(file)\n",
    "                if int(x*100)%2 != 0:\n",
    "                    continue\n",
    "                if int(y*100)%2 != 0:\n",
    "                    continue\n",
    "            FilesImageDataset_fnames.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatGrayImageDataset(Dataset):\n",
    "    def __init__(self, images, targets, exclude=True):\n",
    "        self.images = images\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]  # Get image (NumPy array)\n",
    "        label = self.targets[index]  # Get corresponding tuple\n",
    "        \n",
    "        # Convert image to Tensor, flatten, and normalize [0,1]\n",
    "        image = torch.from_numpy(image).flatten()\n",
    "        image = image.float() / 255.0\n",
    "        \n",
    "        # Convert label tuple to Tensor\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "class FilesImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, filenames):\n",
    "        self.data_dir = data_dir\n",
    "        self.filenames = filenames\n",
    "        self.targets = [imname_to_target(s) for s in filenames]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fname = self.filenames[index]\n",
    "        image = cv2.imread(os.path.join(self.data_dir, fname), cv2.IMREAD_GRAYSCALE)\n",
    "        label = self.targets[index]  # Get corresponding tuple\n",
    "        \n",
    "        # Convert image to Tensor, flatten, and normalize [0,1]\n",
    "        image = torch.from_numpy(image).flatten()\n",
    "        image = image.float() / 255.0\n",
    "        \n",
    "        # Convert label tuple to Tensor\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "class LMDBImageDataset(Dataset):\n",
    "    def __init__(self, lmdb_path, transforms=None, keys_fname=\"keys.txt\", flatten_data=True):\n",
    "        self.keys = None\n",
    "\n",
    "        # Data augmentation\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Read text keys from file\n",
    "        with open(os.path.join(lmdb_path, keys_fname)) as f:\n",
    "            self.keys = f.readlines()\n",
    "            if self.keys[-1] == '':\n",
    "                self.keys = self.keys[:-1]\n",
    "        for i in range(len(self.keys)):\n",
    "            self.keys[i] = self.keys[i].replace(\"\\n\", \"\")\n",
    "\n",
    "        # Get labels from text keys\n",
    "        self.labels = []\n",
    "        # self.labels = [imname_to_target(key) for key in self.keys]\n",
    "        for i, key in enumerate(self.keys):\n",
    "            try:\n",
    "                self.labels.append(imname_to_target(key))\n",
    "            except Exception as e:\n",
    "                print(\"i:\", i)\n",
    "                print(\"name:\", key)\n",
    "                raise e\n",
    "\n",
    "        # Encode keys\n",
    "        for i in range(len(self.keys)):\n",
    "            self.keys[i] = self.keys[i].encode()\n",
    "\n",
    "        self.lmdb_path = lmdb_path\n",
    "        self.flatten_data = flatten_data\n",
    "\n",
    "    def open_lmdb(self):\n",
    "        self.env = lmdb.open(self.lmdb_path, readonly=True, create=False, lock=False, readahead=False, meminit=False)\n",
    "        self.txn = self.env.begin()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def add_coord_channels(self, image_tensor):\n",
    "        _, H, W = image_tensor.shape\n",
    "\n",
    "        x_coords = torch.linspace(-1, 1, W, device=image_tensor.device)\n",
    "        y_coords = torch.linspace(-1, 1, H, device=image_tensor.device)\n",
    "\n",
    "        x_coords = x_coords.view(1, 1, W).expand(1, H, W)\n",
    "        y_coords = y_coords.view(1, H, 1).expand(1, H, W)\n",
    "\n",
    "        return torch.cat([image_tensor, x_coords, y_coords], dim=0)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if not hasattr(self, 'txn'):\n",
    "            print(\"Opening lmdb txn\")\n",
    "            self.open_lmdb()\n",
    "        key = self.keys[index]  # Get corresponding tuple\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        img_bytes = self.txn.get(key)\n",
    "        \n",
    "        if img_bytes is None:\n",
    "            raise KeyError(f\"Image {key} not found in LMDB!\")\n",
    "\n",
    "        if LMDB_USE_COMPRESSION:\n",
    "            img_bytes = lz4.frame.decompress(img_bytes)\n",
    "\n",
    "        image = np.array(msgpack.unpackb(img_bytes, raw=False), dtype=np.uint8)\n",
    "\n",
    "        # Convert image to Tensor, and normalize [0,1]\n",
    "        \n",
    "        if not self.flatten_data:\n",
    "            image = torch.from_numpy(np.array([image])).float()\n",
    "        image = image / 255.0\n",
    "\n",
    "        # Augmenation\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        if self.flatten_data:\n",
    "            image = torch.from_numpy(image).flatten().float()\n",
    "\n",
    "        # image = self.add_coord_channels(image)\n",
    "\n",
    "        # Convert label tuple to Tensor\n",
    "        x, y = label\n",
    "        x = (x + 2) / 5.7\n",
    "        y = (y + 2) / 4\n",
    "        label = (x, y)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = T.Compose([\n",
    "            T.ColorJitter(config[\"jitter_brightness\"],\n",
    "                          config[\"jitter_contrast\"],\n",
    "                        #   config[\"jitter_saturation\"],\n",
    "                        #   config[\"jitter_hue\"]\n",
    "                        ),\n",
    "            T.GaussianNoise(sigma=config[\"noise_level\"]),\n",
    "        ])\n",
    "\n",
    "\n",
    "match config[\"dataset_type\"]:\n",
    "    case \"LMDBImageDataset\":\n",
    "        train_dataset = LMDBImageDataset(config[\"data_folder\"], transforms=train_transforms, flatten_data=config[\"dataset_config_flatten\"], keys_fname=config[\"dataset_train_keys_fname\"])\n",
    "        val_dataset = LMDBImageDataset(config[\"data_folder\"], flatten_data=config[\"dataset_config_flatten\"], keys_fname=config[\"dataset_val_keys_fname\"])\n",
    "    case \"FilesImageDataset\":\n",
    "        train_dataset = FilesImageDataset(config[\"data_folder\"], FilesImageDataset_fnames)\n",
    "        val_dataset = FilesImageDataset(config[\"data_folder\"], FilesImageDataset_fnames)\n",
    "    case _ :\n",
    "        raise(\"Wrong dataset type\")\n",
    "train_data_loader = DataLoader(train_dataset, \n",
    "                         batch_size=config[\"batch_size\"], \n",
    "                         shuffle=True, \n",
    "                         num_workers=8, \n",
    "                         pin_memory=True, \n",
    "                         prefetch_factor=4, \n",
    "                         persistent_workers=True\n",
    "                        )\n",
    "val_data_loader = DataLoader(val_dataset,\n",
    "                             batch_size=config[\"batch_size\"],\n",
    "                             shuffle=False,\n",
    "                             num_workers=4,\n",
    "                             persistent_workers=True,\n",
    "                             pin_memory=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening lmdb txn\n",
      "torch.Size([1, 512, 512])\n",
      "136800\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].shape)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [512, 1, 256, 256]              10\n",
      "              ReLU-2         [512, 1, 256, 256]               0\n",
      "              ReLU-3         [512, 1, 256, 256]               0\n",
      "           Flatten-4               [512, 65536]               0\n",
      "            Linear-5                 [512, 128]       8,388,736\n",
      "       BatchNorm1d-6                 [512, 128]             256\n",
      "              ReLU-7                 [512, 128]               0\n",
      "              ReLU-8                 [512, 128]               0\n",
      "            Linear-9                  [512, 32]           4,128\n",
      "      BatchNorm1d-10                  [512, 32]              64\n",
      "             ReLU-11                  [512, 32]               0\n",
      "             ReLU-12                  [512, 32]               0\n",
      "           Linear-13                   [512, 2]              66\n",
      "================================================================\n",
      "Total params: 8,393,260\n",
      "Trainable params: 8,393,260\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 512.00\n",
      "Forward/backward pass size (MB): 1026.51\n",
      "Params size (MB): 32.02\n",
      "Estimated Total Size (MB): 1570.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, output_size, in_channels):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.sec1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 5, 2), # 3, 250 -> 32, 125\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 32, 125 -> 32, 62\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 2), # 32, 62 -> 64, 31\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 64, 31 -> 64, 15\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 2), # 64, 15 -> 128, 7\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 128, 7 -> 128, 3\n",
    "        )\n",
    "\n",
    "        self.sec2 = nn.Sequential(\n",
    "            nn.Linear(128*3*3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sec1(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.sec2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class SimpleFC(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SimpleFC, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 1024), # 262,144 -> 1024\n",
    "            nn.BatchNorm1d(1024),\n",
    "            self.relu,\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            self.relu,\n",
    "            nn.Linear(256, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            self.relu,\n",
    "            nn.Linear(32, out_features),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers.forward(x)\n",
    "    \n",
    "class GradientSimpleFC(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GradientSimpleFC, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 1024), # 262,144 -> 1024\n",
    "            nn.BatchNorm1d(1024),\n",
    "            self.relu,\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            self.relu,\n",
    "            nn.Linear(256, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            self.relu,\n",
    "            nn.Linear(32, out_features),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers.forward(x)\n",
    "    \n",
    "class SmallFC(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SmallFC, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 128), # 512*512 -> 128\n",
    "            nn.BatchNorm1d(128),\n",
    "            self.relu,\n",
    "            nn.Linear(128, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            self.relu,\n",
    "            nn.Linear(32, out_features),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers.forward(x)\n",
    "    \n",
    "class SmallExt(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SmallExt, self).__init__()\n",
    "        k_size = config[\"conv_kernel\"]\n",
    "        step = config[\"conv_stride\"]\n",
    "        pad = k_size//2\n",
    "        depth = config[\"conv_depth\"]\n",
    "        flat = int((in_features-k_size + 2*pad)//step + 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, depth, k_size, step, padding=pad),\n",
    "            self.relu,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat*flat * depth, 128), # 512*512 -> 128\n",
    "            nn.BatchNorm1d(128),\n",
    "            self.relu,\n",
    "            nn.Linear(128, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            self.relu,\n",
    "            nn.Linear(32, out_features),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers.forward(x)\n",
    "    \n",
    "\n",
    "# model = CoordWideConv().to(DEVICE)\n",
    "# model = SimpleFC(512*512, 2).to(DEVICE)\n",
    "# model = SmallFC(512*512, 2).to(DEVICE)\n",
    "# model = SmallExt(512, 2).to(DEVICE)\n",
    "model = GradientSimpleFC(512, 2).to(DEVICE)\n",
    "\n",
    "summary(model, (1,512,512), config[\"batch_size\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ansemble: \n",
    "- transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), config[\"lr\"], weight_decay=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, config[\"lr_scheduler_loop\"], eta_min=0.00001)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "# scaler = torch.cuda.amp.GradScaler(\"cuda\", enabled=config[\"use_amp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model:torch.nn.Module, fname=\"best_model.pth\", path=\"./saved_models/real\"):\n",
    "    torch.save(model.state_dict(), os.path.join(path,fname))\n",
    "\n",
    "def load_model(model:torch.nn.Module, fname=\"best_model.pth\", path=\"./saved_models/real\"):\n",
    "    model.load_state_dict(torch.load(os.path.join(path,fname), weights_only=False))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma41d74c58ab2f0d2c2bbdb317450ab14a8ad9d4e\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m      3\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultireflection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     name\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwatch(model, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, log_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"a41d74c58ab2f0d2c2bbdb317450ab14a8ad9d4e\")\n",
    "wandb.init(\n",
    "    project=\"multireflection\",\n",
    "    name=config[\"experiment_name\"],\n",
    "    config=config,\n",
    ")\n",
    "wandb.watch(model, log='all', log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer:optim.Optimizer, criterion, scheduler:optim.lr_scheduler.CosineAnnealingWarmRestarts, best_loss=None):\n",
    "    \n",
    "    if best_loss is None:\n",
    "        best_loss = 1000000000\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training Loop\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            # with torch.autocast(device_type=DEVICE, dtype=torch.float16, enabled=config[\"use_amp\"]):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            # scaler.scale(loss).backward()\n",
    "            # scaler.step(optimizer)\n",
    "            # scaler.update()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        last_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            with torch.inference_mode():\n",
    "                out = model(images)\n",
    "                loss = criterion(out, labels)\n",
    "            val_loss += loss.item()\n",
    "        avg_val_loss = val_loss/len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_model = model\n",
    "            best_loss = avg_val_loss\n",
    "            save_model(model, fname=config[\"experiment_name\"]+\"_best_model.pth\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{config['epochs']}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Log Training Loss\n",
    "        wandb.log({\"Train Loss\": avg_train_loss, \"Val Loss\": avg_val_loss, \"LR\": last_lr, \"best_loss\":best_loss})\n",
    "\n",
    "    print(\"Best loss:\", best_loss)\n",
    "    return model, best_model, best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"starting_checkpoint_fname\"] is not None:\n",
    "    model = load_model(model, fname=config[\"starting_checkpoint_fname\"], path=config[\"checkpoint_folder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = None\n",
    "# best_loss = 0.0002398806914137568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 19/268 [01:30<19:49,  4.78s/it] \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, best_model, best_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, scheduler, best_loss)\u001b[0m\n\u001b[1;32m     13\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1806\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1804\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1806\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1809\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/wandb/integration/torch/wandb_torch.py:113\u001b[0m, in \u001b[0;36mTorchHistory.add_log_parameters_hook.<locals>.<lambda>\u001b[0;34m(mod, inp, outp)\u001b[0m\n\u001b[1;32m    110\u001b[0m log_track_params \u001b[38;5;241m=\u001b[39m log_track_init(log_freq)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     hook \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mregister_forward_hook(\n\u001b[0;32m--> 113\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m mod, inp, outp: \u001b[43mparameter_log_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_track_params\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_handles[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prefix] \u001b[38;5;241m=\u001b[39m hook\n\u001b[1;32m    118\u001b[0m     module\u001b[38;5;241m.\u001b[39m_wandb_hook_names\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prefix)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/wandb/integration/torch/wandb_torch.py:108\u001b[0m, in \u001b[0;36mTorchHistory.add_log_parameters_hook.<locals>.parameter_log_hook\u001b[0;34m(module, input_, output, log_track)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     data \u001b[38;5;241m=\u001b[39m parameter\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_tensor_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/wandb/integration/torch/wandb_torch.py:254\u001b[0m, in \u001b[0;36mTorchHistory.log_tensor_stats\u001b[0;34m(self, tensor, name)\u001b[0m\n\u001b[1;32m    251\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(tensor_np)\n\u001b[1;32m    252\u001b[0m     bins \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(bins_np)\n\u001b[0;32m--> 254\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m(\n\u001b[1;32m    255\u001b[0m     {name: wandb\u001b[38;5;241m.\u001b[39mHistogram(np_histogram\u001b[38;5;241m=\u001b[39m(tensor\u001b[38;5;241m.\u001b[39mtolist(), bins\u001b[38;5;241m.\u001b[39mtolist()))},\n\u001b[1;32m    256\u001b[0m     commit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    257\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_log'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model, best_model, best_loss = train(model, train_data_loader, val_data_loader, optimizer, criterion, scheduler, best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LR</td><td>██▇▅▃▂▁██▇▅▃▂▁██▇▅▃▂▁██▇▅▃▂▁</td></tr><tr><td>Train Loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>█▄▄▂▂▂▁▄▇▁▂▂▁▁▆▂▂▃▂▁▁▃▅▂▂▃▁▁</td></tr><tr><td>best_loss</td><td>█▄▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LR</td><td>6e-05</td></tr><tr><td>Train Loss</td><td>0.00062</td></tr><tr><td>Val Loss</td><td>0.0007</td></tr><tr><td>best_loss</td><td>0.0007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mixed_002step_smallext1_512bs_00001lr_aug+</strong> at: <a href='https://wandb.ai/e-venediktov-university-of-pittsburgh/multireflection/runs/e0qircuu' target=\"_blank\">https://wandb.ai/e-venediktov-university-of-pittsburgh/multireflection/runs/e0qircuu</a><br> View project at: <a href='https://wandb.ai/e-venediktov-university-of-pittsburgh/multireflection' target=\"_blank\">https://wandb.ai/e-venediktov-university-of-pittsburgh/multireflection</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_184011-e0qircuu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_input(data_folder:str, fnames:list[str], device):\n",
    "    tensors = []\n",
    "    targets = []\n",
    "    original_images = [] # for visualization\n",
    "    for fname in fnames:\n",
    "        img = cv2.imread(data_folder+fname, cv2.IMREAD_GRAYSCALE)\n",
    "        original_images.append(img)\n",
    "        img = torch.from_numpy(img).flatten()\n",
    "        img = img.float() / 255.0\n",
    "        tensors.append(img)\n",
    "\n",
    "        targets.append(imname_to_target(fname))\n",
    "    result_tensor = torch.stack(tensors).to(device)\n",
    "\n",
    "    return result_tensor, targets, original_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 15625])\n",
      "[(-3.0, -2.1), (-2.9, 1.4), (-1.7, 2.0), (-1.1, 1.4), (-0.6, 2.1), (1.3, -0.9), (-0.0, -0.1), (-0.0, 0.0), (0.8, 0.1)]\n"
     ]
    }
   ],
   "source": [
    "# Prepare test images\n",
    "data_folder = \"data/125x125_laser_x4_y6/\"\n",
    "fnames = [\n",
    "    \"x-3.00_y-2.10.jpg\",\n",
    "    \"x-2.90_y1.40.jpg\",\n",
    "    \"x-1.70_y2.00.jpg\",\n",
    "\n",
    "    \"x-1.10_y1.40.jpg\",\n",
    "    \"x-0.60_y2.10.jpg\",\n",
    "    \"x1.30_y-0.90.jpg\",\n",
    "\n",
    "    \"x-0.00_y-0.10.jpg\",\n",
    "    \"x-0.00_y0.00.jpg\",\n",
    "    \"x0.80_y0.10.jpg\",\n",
    "]\n",
    "test_input, keys, original_images = prepare_test_input(data_folder, fnames, DEVICE)\n",
    "print(test_input.shape)\n",
    "print(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Try best model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbest\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m predictions:torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m best\u001b[38;5;241m.\u001b[39mforward(test_input)\n\u001b[1;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best' is not defined"
     ]
    }
   ],
   "source": [
    "# Try best model\n",
    "best.eval()\n",
    "\n",
    "predictions:torch.Tensor = best.forward(test_input)\n",
    "predictions = predictions.detach().cpu().numpy()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(original_images[i], cmap='gray')\n",
    "    ax.set_title(fnames[i])\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Display prediction\n",
    "    ax.text(35,15,f\"x{predictions[i][0]:.2f}_y{predictions[i][1]:.2f}\", color=\"white\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntargets = np.array(train_dataset.targets)\n",
    "print(ntargets.shape)\n",
    "\n",
    "evaluate_on_train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
