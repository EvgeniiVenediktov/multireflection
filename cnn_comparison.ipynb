{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x71652bb66c90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess_images import data_from_folder\n",
    "from tqdm import tqdm\n",
    "from math import log\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchsummary import summary\n",
    "import cv2 \n",
    "import wandb\n",
    "from config import LMDB_USE_COMPRESSION\n",
    "\n",
    "import lmdb\n",
    "import os\n",
    "import msgpack\n",
    "import io\n",
    "import lz4.frame\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imname_to_target(name:str) -> tuple[float]:\n",
    "    \"\"\"Parses image names of format x{x_value}_y{y_value}.jpg\"\"\"\n",
    "    name = name.split('.jpg')[0]\n",
    "    x, y = name.split(\"_\")\n",
    "    x = float(x[1:])\n",
    "    y = float(y[1:5])\n",
    "    return x, y\n",
    "\n",
    "def save_model(model:torch.nn.Module, fname=\"best_model.pth\", path=\"./saved_models/real\"):\n",
    "    torch.save(model.state_dict(), os.path.join(path,fname))\n",
    "\n",
    "def load_model(model:torch.nn.Module, fname=\"best_model.pth\", path=\"./saved_models/real\"):\n",
    "    model.load_state_dict(torch.load(os.path.join(path,fname), weights_only=False))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"experiment_name\": \"004step_ResNet18_DarkOnly512_lmdb_100bs_0001lr_aug+\",\n",
    "    \"batch_size\": 100,\n",
    "    \"lr\": 0.001,\n",
    "    \"lr_scheduler_loop\": 7,\n",
    "    \"epochs\": 28,\n",
    "    \"use_amp\": False,\n",
    "\n",
    "    # \"data_folder\": \"/mnt/h/real_512_0_001step.lmdb\",\n",
    "    \"data_folder\": \"/mnt/h/real_512_0_004step_tensor.lmdb\",\n",
    "    # \"data_folder\": \"/mnt/e/color.lmdb\",\n",
    "    \"dataset_type\": \"LMDBImageDataset\",\n",
    "    \"dataset_config_flatten\": False,\n",
    "    \"dataset_train_keys_fname\": \"004_dark_train.txt\",\n",
    "    \"dataset_val_keys_fname\": \"004_dark_val.txt\",\n",
    "    \"dataset_offload_count\": 0,\n",
    "\n",
    "    \"use_noise_transform\": True,\n",
    "    \"noise_level\": 0.1,\n",
    "    \"use_jitter_transform\": True,\n",
    "    \"jitter_brightness\": 0.4, \n",
    "    \"jitter_contrast\": 0.1, \n",
    "    \"jitter_saturation\": 0.1, \n",
    "    \"jitter_hue\": 0.2,\n",
    "\n",
    "    \"use_grayscale_transform\": False,\n",
    "    \"use_clahegrad_transform\": False,\n",
    "    \"clahe_clip_limit\": 0.001,\n",
    "    \"clahe_gaussian_size\": 15,\n",
    "    \"clahe_gaussian_sigma\": 5,\n",
    "\n",
    "    \"use_high_pass_transform\": False,\n",
    "    \"high_pass_transform_t\": 0.35,\n",
    "\n",
    "    \"data_collection_step\": 0.001,\n",
    "    \"starting_checkpoint_fname\": None,\n",
    "    \"checkpoint_folder\": \"./saved_models/real\",\n",
    "\n",
    "    \"gradient_layer_kernel_size\": 15,\n",
    "    \"gradient_layer_sigma\": 5,\n",
    "\n",
    "    \"use_weight_initialization\": True,\n",
    "    \"init_red_filter\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InMemoryLMDBImageDataset(Dataset):\n",
    "    def __init__(self, data_folder_path, transforms=None, keys_fname=\"keys.txt\", flatten_data=True, turn_to_grayscale=True):\n",
    "        self.keys = None\n",
    "\n",
    "        # Data augmentation\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Read text keys from file\n",
    "        with open(os.path.join(data_folder_path, keys_fname)) as f:\n",
    "            self.keys = f.readlines()\n",
    "            if self.keys[-1] == '':\n",
    "                self.keys = self.keys[:-1]\n",
    "        for i in range(len(self.keys)):\n",
    "            self.keys[i] = self.keys[i].replace(\"\\n\", \"\")\n",
    "\n",
    "        # Get labels from text keys\n",
    "        self.labels = []\n",
    "        for i, key in enumerate(self.keys):\n",
    "            try:\n",
    "                label = imname_to_target(key)\n",
    "\n",
    "                # Convert label tuple to Tensor\n",
    "                x, y = label\n",
    "                x = (x + 2) / 5.7\n",
    "                y = (y + 2) / 4\n",
    "                label = (x, y)\n",
    "                label = torch.tensor(label, dtype=torch.float32)\n",
    "                self.labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(\"i:\", i)\n",
    "                print(\"name:\", key)\n",
    "                raise e\n",
    "            \n",
    "        # Encode keys\n",
    "        for i in range(len(self.keys)):\n",
    "            self.keys[i] = self.keys[i].encode()\n",
    "\n",
    "        # Load images\n",
    "        self.env = lmdb.open(data_folder_path, readonly=True, create=False, lock=False, readahead=False, meminit=False)\n",
    "        self.txn = self.env.begin()\n",
    "\n",
    "        self.images = [None]*len(self.keys)\n",
    "        self.loaded_indexes = set()\n",
    "        self.flatten_data = flatten_data\n",
    "        self.turn_to_grayscale = turn_to_grayscale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def get_index(self, key):\n",
    "        for i, k in enumerate(self.keys):\n",
    "            if k == key:\n",
    "                return i\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if index in self.loaded_indexes:\n",
    "            img = self.images[index]     \n",
    "        else:\n",
    "            key = self.keys[index]\n",
    "            img_bytes = self.txn.get(key)\n",
    "        \n",
    "            if img_bytes is None:\n",
    "                raise KeyError(f\"Image {key} not found in LMDB!\")\n",
    "\n",
    "\n",
    "            image = torch.asarray(lmdb_bytes_to_torch_tensor(img_bytes), dtype=torch.float32)\n",
    "            self.images[index] = image\n",
    "            self.loaded_indexes.add(index)\n",
    "\n",
    "\n",
    "        # Augmenation\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        if self.flatten_data:\n",
    "            img = img.flatten().float()\n",
    "            self.debug_msg = f\"image shape {img.shape}\"\n",
    "        elif isinstance(img, np.ndarray):\n",
    "            img = torch.unsqueeze(torch.from_numpy(img), 0)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def lmdb_bytes_to_torch_tensor(img_bytes: bytes) -> torch.Tensor:\n",
    "    if img_bytes is None:\n",
    "        raise ValueError(\"img_bytes is None\")\n",
    "\n",
    "    buf = io.BytesIO(img_bytes)\n",
    "    buf.seek(0)\n",
    "    obj = torch.load(buf, map_location='cpu')\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        t = obj\n",
    "    elif isinstance(obj, dict) and 'tensor' in obj and isinstance(obj['tensor'], torch.Tensor):\n",
    "        t = obj['tensor']\n",
    "    return t.contiguous()\n",
    "\n",
    "\n",
    "class LMDBImageDataset(Dataset):\n",
    "    def __init__(self, lmdb_path, transforms=None, keys_fname=\"keys.txt\", flatten_data=True):\n",
    "        self.keys = None\n",
    "\n",
    "        # Data augmentation\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Read text keys from file\n",
    "        with open(os.path.join(lmdb_path, keys_fname)) as f:\n",
    "            self.keys = f.readlines()\n",
    "            if self.keys[-1] == '':\n",
    "                self.keys = self.keys[:-1]\n",
    "        for i in range(len(self.keys)):\n",
    "            self.keys[i] = self.keys[i].replace(\"\\n\", \"\")\n",
    "\n",
    "        # Get labels from text keys\n",
    "        self.labels = []\n",
    "        # self.labels = [imname_to_target(key) for key in self.keys]\n",
    "        for i, key in enumerate(self.keys):\n",
    "            try:\n",
    "                self.labels.append(imname_to_target(key))\n",
    "            except Exception as e:\n",
    "                print(\"i:\", i)\n",
    "                print(\"name:\", key)\n",
    "                raise e\n",
    "\n",
    "        # Encode keys\n",
    "        for i in range(len(self.keys)):\n",
    "            self.keys[i] = self.keys[i].encode()\n",
    "\n",
    "        self.lmdb_path = lmdb_path\n",
    "        self.flatten_data = flatten_data\n",
    "\n",
    "    def open_lmdb(self):\n",
    "        self.env = lmdb.open(self.lmdb_path, readonly=True, create=False, lock=False, readahead=False, meminit=False)\n",
    "        self.txn = self.env.begin()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def get_index(self, key):\n",
    "        for i, k in enumerate(self.keys):\n",
    "            if k == key:\n",
    "                return i\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if not hasattr(self, 'txn'):\n",
    "            print(\"Opening lmdb txn\")\n",
    "            self.open_lmdb()\n",
    "        key = self.keys[index]  # Get corresponding tuple\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        img_bytes = self.txn.get(key)\n",
    "        \n",
    "        if img_bytes is None:\n",
    "            raise KeyError(f\"Image {key} not found in LMDB!\")\n",
    "\n",
    "\n",
    "        image = torch.asarray(lmdb_bytes_to_torch_tensor(img_bytes), dtype=torch.float32)\n",
    "        \n",
    "        # Augmenation\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "            # print(f\"image shaep after transforms: {image.shape}\")\n",
    "        if self.flatten_data:\n",
    "            image = image.flatten().float()\n",
    "            self.debug_msg = f\"image shape {image.shape}\"\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            image = torch.unsqueeze(torch.from_numpy(image), 0)\n",
    "            # print(image.shape)\n",
    "\n",
    "        # Convert label tuple to Tensor\n",
    "        x, y = label\n",
    "        x = (x + 2) / 5.7\n",
    "        y = (y + 2) / 4\n",
    "        label = (x, y)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarr = []\n",
    "\n",
    "if config[\"use_jitter_transform\"]:\n",
    "    tarr.append(\n",
    "        T.ColorJitter(\n",
    "            config[\"jitter_brightness\"],\n",
    "            config[\"jitter_contrast\"],\n",
    "            # config[\"jitter_saturation\"],\n",
    "            # config[\"jitter_hue\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "if config[\"use_noise_transform\"]:\n",
    "    tarr.append(\n",
    "        T.GaussianNoise(sigma=config[\"noise_level\"]),\n",
    "    )\n",
    "\n",
    "varr = []\n",
    "\n",
    "if config[\"use_grayscale_transform\"]:\n",
    "    tarr.append(T.Grayscale())\n",
    "    varr.append(T.Grayscale())\n",
    "\n",
    "train_transforms = T.Compose(tarr)\n",
    "val_transforms = T.Compose(varr) if len(varr)>0 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      ColorJitter(brightness=(0.6, 1.4), contrast=(0.9, 1.1))\n",
       "      GaussianNoise(mean=0.0, sigma=0.1, clip=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "match config[\"dataset_type\"]:\n",
    "    case \"LMDBImageDataset\":\n",
    "        train_dataset = LMDBImageDataset(config[\"data_folder\"], transforms=train_transforms, flatten_data=config[\"dataset_config_flatten\"], keys_fname=config[\"dataset_train_keys_fname\"])\n",
    "        val_dataset = LMDBImageDataset(config[\"data_folder\"], transforms=val_transforms, flatten_data=config[\"dataset_config_flatten\"], keys_fname=config[\"dataset_val_keys_fname\"])\n",
    "\n",
    "    case \"InMemoryImageDataset\":\n",
    "        train_dataset = InMemoryLMDBImageDataset(config[\"data_folder\"], transforms=train_transforms, flatten_data=config[\"dataset_config_flatten\"], keys_fname=config[\"dataset_train_keys_fname\"])\n",
    "        val_dataset = InMemoryLMDBImageDataset(config[\"data_folder\"], transforms=val_transforms, flatten_data=config[\"dataset_config_flatten\"], keys_fname=config[\"dataset_val_keys_fname\"])\n",
    "    case _ :\n",
    "        raise(\"Wrong dataset type\")\n",
    "train_data_loader = DataLoader(train_dataset, \n",
    "                         batch_size=config[\"batch_size\"], \n",
    "                         shuffle=True, \n",
    "                         num_workers=8, \n",
    "                         pin_memory=True, \n",
    "                         prefetch_factor=4, \n",
    "                         persistent_workers=True\n",
    "                        )\n",
    "val_data_loader = DataLoader(val_dataset,\n",
    "                             batch_size=config[\"batch_size\"],\n",
    "                             shuffle=False,\n",
    "                             num_workers=4,\n",
    "                             persistent_workers=True,\n",
    "                             pin_memory=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening lmdb txn\n",
      "torch.Size([1, 512, 512])\n",
      "11440\n",
      "Compose(\n",
      "      ColorJitter(brightness=(0.6, 1.4), contrast=(0.9, 1.1))\n",
      "      GaussianNoise(mean=0.0, sigma=0.1, clip=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].shape)\n",
    "print(len(train_dataset))\n",
    "print(train_dataset.transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'x0.40_y-1.96.jpg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample, _ = train_dataset[train_dataset.get_index(b'x0.00_y0.00.jpg')]\n",
    "# sample = sample.squeeze().cpu().numpy().reshape((512, 512))\n",
    "\n",
    "# print(sample.shape)\n",
    "# plt.imsave(\"x0.00_y0.00.jpg\", sample, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(3,1)\n",
    "# print(\"Original\")\n",
    "# axes[0].imshow(train_dataset[0][0].permute(1,2,0).numpy())\n",
    "# axes[0].set_title(train_dataset.keys[0])\n",
    "# axes[1].imshow(train_dataset[1][0].permute(1,2,0).numpy())\n",
    "# axes[1].set_title(train_dataset.keys[1])\n",
    "# axes[2].imshow(train_dataset[10][0].permute(1,2,0).numpy())\n",
    "# axes[2].set_title(train_dataset.keys[10])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144\n",
      "31827\n",
      "4900\n",
      "1152\n",
      "784\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "def size_after_conv(input_size, kernel_size, stride, padding):\n",
    "    return (input_size - kernel_size + 2 * padding) // stride + 1\n",
    "conv_config = [\n",
    "    {'out_channels':3, 'kernel_size':150, 'stride':5},\n",
    "    {'out_channels':4, 'kernel_size':75, 'stride':3},\n",
    "    {'out_channels':8, 'kernel_size':30, 'stride':3},\n",
    "    {'out_channels':16, 'kernel_size':6, 'stride':2},\n",
    "    {'out_channels':32, 'kernel_size':3, 'stride':2},\n",
    "]\n",
    "\n",
    "for l in conv_config:\n",
    "    l['padding'] = l['kernel_size'] // 2\n",
    "\n",
    "\n",
    "s = 512\n",
    "print(s*s)\n",
    "for l in conv_config:\n",
    "    if l is not None:\n",
    "        s = size_after_conv(s, l['kernel_size'], l['stride'], l['padding'])\n",
    "        print(s*s*l['out_channels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['conv_config'] = conv_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic ResNet block for ResNet-18 and ResNet-34\"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Bottleneck block for ResNet-50, ResNet-101, and ResNet-152\"\"\"\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, output_dim=2):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        # Final layers\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, output_dim)\n",
    "        \n",
    "        # Weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                         kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Factory functions for different ResNet variants\n",
    "def resnet18(output_dim=2):\n",
    "    \"\"\"ResNet-18 model\"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], output_dim=output_dim)\n",
    "\n",
    "\n",
    "def resnet34(output_dim=2):\n",
    "    \"\"\"ResNet-34 model\"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], output_dim=output_dim)\n",
    "\n",
    "\n",
    "def resnet50(output_dim=2):\n",
    "    \"\"\"ResNet-50 model\"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], output_dim=output_dim)\n",
    "\n",
    "\n",
    "def resnet101(output_dim=2):\n",
    "    \"\"\"ResNet-101 model\"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], output_dim=output_dim)\n",
    "\n",
    "\n",
    "def resnet152(output_dim=2):\n",
    "    \"\"\"ResNet-152 model\"\"\"\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], output_dim=output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [100, 64, 256, 256]           3,136\n",
      "       BatchNorm2d-2        [100, 64, 256, 256]             128\n",
      "              ReLU-3        [100, 64, 256, 256]               0\n",
      "         MaxPool2d-4        [100, 64, 128, 128]               0\n",
      "            Conv2d-5        [100, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-6        [100, 64, 128, 128]             128\n",
      "              ReLU-7        [100, 64, 128, 128]               0\n",
      "            Conv2d-8        [100, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-9        [100, 64, 128, 128]             128\n",
      "             ReLU-10        [100, 64, 128, 128]               0\n",
      "       BasicBlock-11        [100, 64, 128, 128]               0\n",
      "           Conv2d-12        [100, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-13        [100, 64, 128, 128]             128\n",
      "             ReLU-14        [100, 64, 128, 128]               0\n",
      "           Conv2d-15        [100, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-16        [100, 64, 128, 128]             128\n",
      "             ReLU-17        [100, 64, 128, 128]               0\n",
      "       BasicBlock-18        [100, 64, 128, 128]               0\n",
      "           Conv2d-19         [100, 128, 64, 64]          73,728\n",
      "      BatchNorm2d-20         [100, 128, 64, 64]             256\n",
      "             ReLU-21         [100, 128, 64, 64]               0\n",
      "           Conv2d-22         [100, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-23         [100, 128, 64, 64]             256\n",
      "           Conv2d-24         [100, 128, 64, 64]           8,192\n",
      "      BatchNorm2d-25         [100, 128, 64, 64]             256\n",
      "             ReLU-26         [100, 128, 64, 64]               0\n",
      "       BasicBlock-27         [100, 128, 64, 64]               0\n",
      "           Conv2d-28         [100, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-29         [100, 128, 64, 64]             256\n",
      "             ReLU-30         [100, 128, 64, 64]               0\n",
      "           Conv2d-31         [100, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-32         [100, 128, 64, 64]             256\n",
      "             ReLU-33         [100, 128, 64, 64]               0\n",
      "       BasicBlock-34         [100, 128, 64, 64]               0\n",
      "           Conv2d-35         [100, 256, 32, 32]         294,912\n",
      "      BatchNorm2d-36         [100, 256, 32, 32]             512\n",
      "             ReLU-37         [100, 256, 32, 32]               0\n",
      "           Conv2d-38         [100, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-39         [100, 256, 32, 32]             512\n",
      "           Conv2d-40         [100, 256, 32, 32]          32,768\n",
      "      BatchNorm2d-41         [100, 256, 32, 32]             512\n",
      "             ReLU-42         [100, 256, 32, 32]               0\n",
      "       BasicBlock-43         [100, 256, 32, 32]               0\n",
      "           Conv2d-44         [100, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-45         [100, 256, 32, 32]             512\n",
      "             ReLU-46         [100, 256, 32, 32]               0\n",
      "           Conv2d-47         [100, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-48         [100, 256, 32, 32]             512\n",
      "             ReLU-49         [100, 256, 32, 32]               0\n",
      "       BasicBlock-50         [100, 256, 32, 32]               0\n",
      "           Conv2d-51         [100, 512, 16, 16]       1,179,648\n",
      "      BatchNorm2d-52         [100, 512, 16, 16]           1,024\n",
      "             ReLU-53         [100, 512, 16, 16]               0\n",
      "           Conv2d-54         [100, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-55         [100, 512, 16, 16]           1,024\n",
      "           Conv2d-56         [100, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-57         [100, 512, 16, 16]           1,024\n",
      "             ReLU-58         [100, 512, 16, 16]               0\n",
      "       BasicBlock-59         [100, 512, 16, 16]               0\n",
      "           Conv2d-60         [100, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-61         [100, 512, 16, 16]           1,024\n",
      "             ReLU-62         [100, 512, 16, 16]               0\n",
      "           Conv2d-63         [100, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-64         [100, 512, 16, 16]           1,024\n",
      "             ReLU-65         [100, 512, 16, 16]               0\n",
      "       BasicBlock-66         [100, 512, 16, 16]               0\n",
      "AdaptiveAvgPool2d-67           [100, 512, 1, 1]               0\n",
      "           Linear-68                   [100, 2]           1,026\n",
      "================================================================\n",
      "Total params: 11,171,266\n",
      "Trainable params: 11,171,266\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 100.00\n",
      "Forward/backward pass size (MB): 32800.39\n",
      "Params size (MB): 42.61\n",
      "Estimated Total Size (MB): 32943.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConfigCNN(nn.Module):\n",
    "    def __init__(self, output_size = 2, input_size=(1, 250, 250)):\n",
    "        super(ConfigCNN, self).__init__()\n",
    "        c, h, w = input_size\n",
    "        layers = []\n",
    "        prev_channels = c\n",
    "        size = c * h * w\n",
    "        for layer_config in conv_config:\n",
    "            layers.append(\n",
    "                nn.Conv2d(prev_channels, \n",
    "                          layer_config['out_channels'], \n",
    "                          layer_config['kernel_size'], \n",
    "                          layer_config['stride'], \n",
    "                          padding=layer_config['padding']\n",
    "                          )\n",
    "            )\n",
    "            prev_channels = layer_config['out_channels']\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm2d(layer_config['out_channels']))\n",
    "\n",
    "            h = size_after_conv(h, layer_config['kernel_size'], layer_config['stride'], layer_config['padding'])\n",
    "            w = h\n",
    "            c = layer_config['out_channels']\n",
    "            size = c * h * w\n",
    "\n",
    "        self.sec1 = nn.Sequential(\n",
    "            *layers\n",
    "        )\n",
    "\n",
    "        self.sec2 = nn.Sequential(\n",
    "            # nn.Linear(size, size//4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.BatchNorm1d(size//4),\n",
    "            nn.Linear(size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sec1(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.sec2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleFC(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SimpleFC, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 1024), # 262,144 -> 1024\n",
    "            nn.BatchNorm1d(1024),\n",
    "            self.relu,\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            self.relu,\n",
    "            nn.Linear(256, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            self.relu,\n",
    "            nn.Linear(32, out_features),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers.forward(x)\n",
    "    \n",
    "    \n",
    "\n",
    "# model = SimpleFC(512*512, 2).to(DEVICE)\n",
    "# model = ConfigCNN(2, input_size=(1, 512, 512)).to(DEVICE)\n",
    "model = resnet18(output_dim=2).to(DEVICE)\n",
    "\n",
    "if config[\"use_weight_initialization\"]:\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d,\n",
    "                          nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "summary(model, (1,512,512), config[\"batch_size\"])\n",
    "# summary(model, (512*512,), config[\"batch_size\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ansemble: \n",
    "- transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), config[\"lr\"], weight_decay=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, config[\"lr_scheduler_loop\"], eta_min=0.00001)\n",
    "# scheduler = optim.lr_scheduler.ConstantLR(optimizer, 1, 0, )\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "# scaler = torch.cuda.amp.GradScaler(\"cuda\", enabled=config[\"use_amp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/evv/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33me-venediktov\u001b[0m (\u001b[33me-venediktov-university-of-pittsburgh\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/EVV13/Documents/multireflection/wandb/run-20251217_141428-rxf95qrv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/e-venediktov-university-of-pittsburgh/multireflection/runs/rxf95qrv' target=\"_blank\">004step_ResNet18_DarkOnly512_lmdb_100bs_0001lr_aug+</a></strong> to <a href='https://wandb.ai/e-venediktov-university-of-pittsburgh/multireflection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/e-venediktov-university-of-pittsburgh/multireflection' target=\"_blank\">https://wandb.ai/e-venediktov-university-of-pittsburgh/multireflection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/e-venediktov-university-of-pittsburgh/multireflection/runs/rxf95qrv' target=\"_blank\">https://wandb.ai/e-venediktov-university-of-pittsburgh/multireflection/runs/rxf95qrv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(key=\"a41d74c58ab2f0d2c2bbdb317450ab14a8ad9d4e\")\n",
    "wandb.init(\n",
    "    project=\"multireflection\",\n",
    "    name=config[\"experiment_name\"],\n",
    "    config=config,\n",
    "    resume=\"allow\",\n",
    ")\n",
    "wandb.watch(model, log='all', log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer: optim.Optimizer, criterion, scheduler: optim.lr_scheduler.CosineAnnealingWarmRestarts, best_loss=None):\n",
    "    scaler = GradScaler(DEVICE)\n",
    "    if best_loss is None:\n",
    "        best_loss = 1000000000\n",
    "    best_model = None\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(\"cuda\", dtype=torch.float16, enabled=False):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        last_lr = scheduler.get_last_lr()[0]\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.inference_mode():\n",
    "            for images, labels in tqdm(val_loader):\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                with autocast(\"cuda\", dtype=torch.float16, enabled=False):\n",
    "                    out = model(images)\n",
    "                    loss = criterion(out, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_model = model\n",
    "            best_loss = avg_val_loss\n",
    "            save_model(model, fname=config[\"experiment_name\"] + \"_best_model.pth\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{config['epochs']}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Log Training Loss\n",
    "        log_train_loss = log(avg_train_loss)\n",
    "        log_val_loss = log(avg_val_loss)\n",
    "        avg_total_loss = avg_train_loss * 0.8 + avg_val_loss * 0.2\n",
    "        log_total_loss = log(avg_total_loss)\n",
    "        wandb.log({\n",
    "            \"Train Loss\": avg_train_loss,\n",
    "            \"Val Loss\": avg_val_loss,\n",
    "            \"LR\": last_lr,\n",
    "            \"best_loss\": best_loss,\n",
    "            \"log_train_loss\": log_train_loss,\n",
    "            \"log_val_loss\": log_val_loss,\n",
    "            \"avg_total_loss\": avg_total_loss,\n",
    "            \"log_total_loss\": log_total_loss,\n",
    "            # \"ds_train_loaded\": len(train_dataset.loaded_indexes),\n",
    "            # \"ds_val_loaded\": len(val_dataset.loaded_indexes),\n",
    "        })\n",
    "\n",
    "    print(\"Best loss:\", best_loss)\n",
    "    return model, best_model, best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"starting_checkpoint_fname\"] is not None:\n",
    "    model = load_model(model, fname=config[\"starting_checkpoint_fname\"], path=config[\"checkpoint_folder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = None\n",
    "# best_loss = 0.0115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [28:50<00:00, 15.05s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening lmdb txnOpening lmdb txnOpening lmdb txn\n",
      "\n",
      "\n",
      "Opening lmdb txn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [01:00<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28, Train Loss: 0.2014, Val Loss: 0.0854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [31:11<00:00, 16.27s/it]\n",
      "100%|██████████| 29/29 [00:56<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/28, Train Loss: 0.0127, Val Loss: 0.3616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [34:10<00:00, 17.83s/it]\n",
      "100%|██████████| 29/29 [00:58<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/28, Train Loss: 0.0090, Val Loss: 0.5080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [29:46<00:00, 15.54s/it]\n",
      "100%|██████████| 29/29 [00:45<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/28, Train Loss: 0.0068, Val Loss: 0.1326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [33:24<00:00, 17.43s/it]\n",
      "100%|██████████| 29/29 [00:59<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/28, Train Loss: 0.0051, Val Loss: 0.2577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [32:05<00:00, 16.74s/it]\n",
      "100%|██████████| 29/29 [00:58<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/28, Train Loss: 0.0040, Val Loss: 0.2063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [31:14<00:00, 16.30s/it]\n",
      "100%|██████████| 29/29 [00:45<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/28, Train Loss: 0.0034, Val Loss: 0.2391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [30:13<00:00, 15.77s/it]\n",
      "100%|██████████| 29/29 [00:45<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/28, Train Loss: 0.0076, Val Loss: 0.1988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [32:58<00:00, 17.20s/it]\n",
      "100%|██████████| 29/29 [00:46<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/28, Train Loss: 0.0058, Val Loss: 0.2461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [32:38<00:00, 17.03s/it]\n",
      "100%|██████████| 29/29 [01:00<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/28, Train Loss: 0.0035, Val Loss: 0.1122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 30/115 [09:26<26:46, 18.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, best_model, best_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, scheduler, best_loss)\u001b[0m\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     21\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 22\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     25\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/amp/grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, best_model, best_loss = train(model, train_data_loader, val_data_loader, optimizer, criterion, scheduler, best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.debug_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
